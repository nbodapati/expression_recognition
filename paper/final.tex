\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% Include other packages here, before hyperref.
\usepackage{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

% graphics pull from this folders
\graphicspath{{../figs/}}

\begin{document}
\title{Expression Recognition}

\author{Nagasravika Bodapati \& Grace Yoo \\ 
University of Massachusetts Amherst \\ 
College of Information \& Computer Sciences \\
{\tt\small \{gyoo, nbodapati\}@cs.umass.edu} \\
}

\maketitle

\section{Abstract} 
% no more than 300 words
For human interactions, an estimated $55\%$ of information is conveyed via emotional cues and the rest is through voice intonations and words used. Thus, using detection techniques to allow machines to recognize emotional states of humans may greatly improve human-computer interactions. For example, a machine might be trained to assess the quality of a lecture or a movie by reading the facial expressions of the audience; seeing many happy faces might suggest the event is interesting, while recognizing bored expressions might suggest the opposite.  \\

\noindent Our objective is to build an expression recognition pipeline that can detect faces in images, extract features from them, and classify the mood or expression of the person. We will experiment with a variety of preprocessing, feature extraction, and modeling methods.

\section{Introduction}
% Introduction: this section introduces facial expression recognition in general
Facial expression recognition aims to automatically determine the emotional state of a person, based on their facial features. The established "basic emotions" to choose from include happiness, sadness, anger, fear, disgust, surprise, or neutral. Within the field there are slight variations on this set of emotions. 

The domain of facial expression recognition has been shaped greatly by social and psychological research on emotional states. The "universality studies" were a series of experiments in the early '70s that aimed to determine whether or not facial expressions are culture-specific. For example, a study showed the same images of emotional expressions of Americans to people from the United States, Brazil, Chile, Argentina, and Japan, and asked participants to guess the emotion. The researchers expected that expressions for emotions such as "anger" might be confused with other negative expressions, such as those for "disgust" or "fear". However, regardless of what country the participants were from, people were very good at recognizing emotions accurately. The consensus across all of the universality studies was that facial expressions are indeed universally recognizable \cite{universality_studies}. More recently, Elfenbein and Ambady published a meta-analysis of 168 studies (including the universality studies) that confirmed emotion recognition of the basic emotions is likely universal \cite{universality}.

These studies provide strong foundations for the domain of facial expression recognition. We know that humans are capable of reliably recognizing basic emotions, and we know that the majority of those signals aren't too culture-specific. So, it's likely that the signals come from the physical facial features.

% facial expression domain-specific
FACS, FACS AU detection, EMFACS, 


\section{Related Work} 
% This section discusses relevant literature for your project
Since our project domain was so broad, we 


\subsection{Data Processing Techniques}
% previous study comparing ROI vs full face

\subsection{Feature Extraction Techniques}
% previous studies for:
% 'pixel', 'pixel_sharpen', 'pixel_gradient'

% 'hog'

% 'lbp' 

% 'gabor'

% 'soft_clustering', 

% 'edge_features' 

% 'fiducial_points'

\subsection{Dimensionality Reduction Techniques}
% PCA
% NMF

% when to use, when not to use
% expectations

\subsection{Classifiers}
% give a good example of each, express expectations we have for our results

% decision tree

% KNN

% SVM

% adaboost

% cluster template matching

% HCRF

% neural nets, ensemble

\section{Approach}
% Approach: This section details the framework of your project. Be specific, which means you might want to include equations, figures, plots, etc

% motivate model pipeline style
We wanted to understand, at a high level, what combinations of pre-processing, feature extraction, dimensionality reduction, and modeling techniques result in the best expression classifiers. Our approach was to build a pipeline with a variety of modular feature extraction and modeling functions, such that we could easily try many models. We took the most promising models, based on test set prediction accuracies, and tuned them. 

% motivate data usage
We performed cross-database evaluation of our methods, using both the JAFFE and Cohn-Kanade datasets. The motivation for this was two-fold; we wanted to corroborate our results as well as explore the effects of using different datasets for emotion recognition. In the experiment from the universality studies mentioned previously, not all emotions were recognized with equal agreement. For example, images depicting happiness had $95-100\%$ agreement across cultures, while images depicting fear and sadness had between $62-85\%$ agreement \cite{universality_studies}. The variability in emotion recognition might be explained in part by "in-group advantage". In-group advantage, in the context of facial expression recognition, describes the improved ability to accurately label emotions expressed by a person of your own ethnic group \cite{universality}. Often, people within an ethnic group have similar physical traits, which may contribute to the in-group advantage. We wondered if training on a more homogeneous set of facial images would improve the ability of classifiers to pick up on clear signals.

%TODO : add all external packages here + then cite them
Our implementation is written in Matlab, and requires the Image Processing, Fuzzy Logic, and the Neural Networks Toolbox. External packages include the Matlab Toolbox for Dimensionality reduction \cite{drtoolbox}.

\subsection{Preprocessing}
Pre-processing stage involves steps taken to make the classification invariant to scaling, translation or rotation

% aligning images using "similarity" transformation and landmarks, 
% cropping
% show result of align + crop for both datasets

An optional pre-processing step is to extract ROIs. 
Finding regions of interest
% show result of ROI

\subsection{Feature Extraction}
% describe the implementations of feature extraction techniques
%  'pixel', 'pixel_norm1', 'pixel_norm2', 'pixel_sharpen', 'pixel_sharpen_norm1', 'pixel_sharpen_norm2',
%  'pixel_gradient', 'pixel_gradient_norm1', 'pixel_gradient_norm2', 'hog', 'hog_norm1', 'hog_norm2', 
%  'lbp', 'lbp_norm1', 'lbp_norm2', 'lbp2', 'gabor', 'gabor_norm1', 'gabor_norm2', 'soft_clustering', 
% 'soft_clustering_norm1', 'soft_clustering_norm2', 'edge_features', 'edge_features_n1', 
% 'edge_features_n2', 'fiducial_points', 'fiducial_points_n1'

\subsection{Classification}

% feature selection / dim reduction
dimensionality reduction to check or reduce computational complexity and overfitting
PCA, NMF

% train/test split

% classifiers
Decision Tree
K-Nearest Neighbors (KNN)
Support Vector Machine (SVM)
Neural Network
ensemble Neural Network
Adaboost
Cluster matching template
HCRF


\section{Experiments} 
% This section begins with what kind of experiments you're doing, what kind of dataset(s) you're using, and what is the way you measure or evaluate your results. It then shows in details the results of your experiments. By details, we mean both quantitative evaluations (show numbers, figures, tables, etc) as well as qualitative results (show images, example results, etc).

\subsection{Data}
The Version 1 of the Cohn-Kanade AU-Coded Facial Expression Database \cite{ck} includes 487 sequences from 97 individuals, who were asked to make facial expressions for the basic emotions. A sequence, which is typically comprised of about 20 photographs, begins with a neutral expression and peaks at an emotional expression. The peak image is not necessarily the last one of the sequence. For each sequence, only the image corresponding to the peak expression is annotated with FACS and labeled with the emotion.


We used the CIFAR-10 dataset, which consists of $60,000$ images evenly distributed into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). Each $32 \times 32 \times 3$ pixel image is represented by the RGB color model, so it has 3 channels. We will remove a $16 \times 16$ region of each image, and consider the original image to be the original distribution of $32 \times 32 \times 3 = 3,072$ dimensions. All images have normalized pixel intensities. \\

This is what our incomplete images look like after we remove the center pixels.\\
%\includegraphics[width=1.0\linewidth]{img_sample.png}



\subsection{Setup}
%% how much in train and test set
%% what combinations of processing, feature extraction, etc were applied and what reasoning?
%% what combinations were avoided?

\subsection{Results}
Table 1 shows the prediction accuracy on a test set for each model.
\begin{table}[!ht]
\centering
\caption{Each of 3 human judges saw 10 samples each, totaling 30 total samples for each model}
\begin{tabular}{lll}
\hline
Model               & Test RMSE & \# Convincing samples \\ \hline
CNN with Regression & 24.468    &  4                     \\
Pixel RNN w Sigmoid &   -       &  -                     \\
Pixel RNN w Softmax &   -       &  10                     \\
Pixel CNN           &   -       &  -                     \\ \hline
\end{tabular}
\end{table}

\section{Conclusion}
% Conclusion: What have you learned? Suggest future ideas.
%TODO : come up with high level conclusions
%TODO : future ideas


\begin{thebibliography}{1}
% datasets
\bibitem{ck} Comprehensive Database for Facial Expression Analysis. Kanade, Cohn, Tian.
\bibitem{ck_extended} The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression.
\bibitem{jaffe} MMI Facial Expression Database \url{http://mmifacedb.eu/}

% git repos
\bibitem{drtoolbox} Matlab Toolbox for Dimensionality Reduction \url{https://lvdmaaten.github.io/drtoolbox/}

% chapter in book
\bibitem{universality_studies} chapt 4 Facial Expression

% papers
\bibitem{metastudy} Meta-Analyis of the First Facial Expression Recognition Challenge. 2002. Valstar et al.
\bibitem{universality} Elfenbein, H. A., \& Ambady, N. (2002a). On the universality and cultural specificity of emotion recognition: A meta-analysis. Psychological Bulletin, 128(2), 205-235. 
\bibitem{preprocess} Preprocessing of Face Images: Detection of Features and Pose. 1998. D Reisfeld and Yehezkel Yshurun.
\bibitem{committeeNN} Facial expression (mood) recognition from facial images using committee neural networks. Saket S Kulkarni, Narender P Reddy and SI Hariharan
\bibitem{tutorial} Facial Expression Recognition: A Brief Tutorial Overview. Claude C. Chibelushi, Fabrice Bourel.
\end{thebibliography}

\end{document}